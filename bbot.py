import os
import json
from datetime import datetime
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_upstage import UpstageEmbeddings
from typing import List
from pydantic import BaseModel, Field
from typing_extensions import TypedDict
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from openai import OpenAI


load_dotenv()
base_url = os.getenv("SUPABASE_URL")

# ğŸ”¹ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
api_key = os.getenv("UPSTAGE_API_KEY")
api_key = os.environ["UPSTAGE_API_KEY"]
base_url = os.environ["UPSTAGE_BASE_URL"]

# ğŸ”¹ Upstage ëª¨ë¸

model = OpenAI(api_key=api_key, base_url=base_url)

# =========================
# DB ìƒì„± ë° ë¬¸ì„œ ì²˜ë¦¬
# =========================
def create_db(metas: List[dict], persist_dir: str = "./chroma_db") -> Chroma:
    """ë©”íƒ€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Chroma VectorStore ìƒì„±"""
    docs = [
        Document(
            page_content=m["Content"],
            metadata={"URL": m["URL"], "Title": m["Title"], "CrawlTime": m["CrawlTime"]}
        )
        for m in metas
    ]
    # í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(chunk_size=3500, chunk_overlap=100)
    split_docs = splitter.split_documents(docs)
    
    # ì„ë² ë”© ëª¨ë¸
    embedding_model = UpstageEmbeddings(upstage_api_key=api_key, model="embedding-query")
    
    # Chroma DB
    db = Chroma.from_documents(documents=split_docs, embedding=embedding_model, persist_directory=persist_dir)
    return db


# =========================
# ì–¸ì–´ ìë™ ê°ì§€ í•¨ìˆ˜
# =========================
def detect_language(text: str) -> str:
    """í•œêµ­ì–´ í¬í•¨ ì—¬ë¶€ë¡œ ì–¸ì–´ ê°ì§€"""
    if any('\uac00' <= ch <= '\ud7a3' for ch in text):
        return "ko"
    return "en"


# =========================
# RAG ê´€ë ¨ í•¨ìˆ˜
# =========================
def web_search(question: str) -> str:
    # ì›¹ ê²€ìƒ‰ placeholder
    return "ì›¹ ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸"

def generate(question: str) -> str:
    lang = detect_language(question)

    if lang == "ko":
        lang_instruction = "ì‚¬ìš©ì ì§ˆë¬¸ì´ í•œêµ­ì–´ì´ë¯€ë¡œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”."
    else:
        lang_instruction = "The user asked in English, so answer naturally and fluently in English."

    system_prompt = f"""
    ë‹¹ì‹ ì€ ê¸°ë…êµì  ê´€ì ì—ì„œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì§ˆë¬¸ì´ ì¼ë°˜ì ì´ê±°ë‚˜ ê³¼í•™ì ì´ì–´ë„, ë‹µë³€ì— ë°˜ë“œì‹œ ì„±ê²½ì  ë˜ëŠ” ê¸°ë…êµì  ê´€ì ì„ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

    {lang_instruction}
    """

    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ],
        temperature=0
    )
    return response.choices[0].message.content

# =========================
# Question Rewriting
# =========================
system = """You are a question re-writer that converts an input question
to a better version optimized for vectorstore retrieval.
Analyze the input and reason about the underlying semantic intent/need of the question, then rewrite it accordingly."""

prompt_rewriter = ChatPromptTemplate.from_messages([
    ("system", system),
    ("human", "Original question: {question}")
])

def upstage_rewriter(prompt_value):
    prompt_text = prompt_value.to_string()
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "system", "content": system}, {"role": "user", "content": prompt_text}],
        temperature=0
    )
    return response.choices[0].message.content

chain_rewriter = prompt_rewriter | RunnableLambda(upstage_rewriter) | StrOutputParser()

# =========================
# Relevancy íŒë‹¨
# =========================
class Relevancy(BaseModel):
    judgement: str
    binary_score: str

def is_relevant(question: str, document: str) -> Relevancy:
    system_prompt = """You are an expert judge assessing the relevance of a document to a user question.
Respond strictly in valid JSON only."""
    prompt = f"""{system_prompt}\n\nRetrieved document:\n{document}\n\nUser question:\n{question}\n\nRespond in JSON format: {{"judgement": "relevant"/"not_relevant","binary_score": "yes"/"no"}}"""
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    data = json.loads(response.choices[0].message.content)
    return Relevancy(**data)

# =========================
# Factfulness íŒë‹¨
# =========================
class Factfulness(BaseModel):
    judgement: str
    binary_score: str

def check_factfulness(document: str, generation: str) -> Factfulness:
    system_prompt = """You are a judge assessing whether an LLM generation is grounded in a set of retrieved documents.
Respond strictly in valid JSON only."""
    prompt = f"{system_prompt}\n\nSet of facts:\n{document}\n\nLLM generation:\n{generation}\n\nRespond in JSON format: {{'judgement':'factual'/'hallucinated','binary_score':'yes'/'no'}}"
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    data = json.loads(response.choices[0].message.content)
    return Factfulness(**data)

# =========================
# State ì •ì˜
# =========================
class State(TypedDict):
    question: str
    generation: str
    documents: List[str]
    source: str