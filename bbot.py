import os
import json
from datetime import datetime
from typing import List
from pydantic import BaseModel
from typing_extensions import TypedDict
from dotenv import load_dotenv
from openai import OpenAI
import psycopg2
from langchain_upstage import UpstageEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# ğŸ”¹ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("UPSTAGE_API_KEY")
base_url = os.getenv("UPSTAGE_BASE_URL")

# ğŸ”¹ Upstage ëª¨ë¸
model = OpenAI(api_key=api_key, base_url=base_url)
embedding_model = UpstageEmbeddings(upstage_api_key=api_key, model="embedding-query")

# =========================
# DB ìƒì„± ë° ë¬¸ì„œ ì²˜ë¦¬
# =========================
def create_db(metas: List[dict], db_name: str = "bbot_db") -> None:
    """ë©”íƒ€ ë°ì´í„°ë¥¼ PostgreSQL DBì— ì €ì¥, content_embeddingì€ vectorë¡œ ì €ì¥"""
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        dbname=db_name,
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD"),
        port=os.getenv("DB_PORT")
    )
    cur = conn.cursor()
    print("DB ì—°ê²° ì„±ê³µ")

    # pgvector í™•ì¥ ìƒì„±
    cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")

    # í…Œì´ë¸” ìƒì„±
    cur.execute("""
    CREATE TABLE IF NOT EXISTS crawled_data (
        id SERIAL PRIMARY KEY,
        title TEXT,
        url TEXT,
        crawl_time TIMESTAMP,
        content TEXT,
        content_embedding vector(4096)
    );
    """)

    # DBì— ë°ì´í„° ì‚½ì…
    for m in metas:
        title = m.get("Title", "")
        url = m.get("URL", "")
        crawl_time = m.get("CrawlTime", datetime.now())
        content = m.get("Content", "")

        if isinstance(crawl_time, str):
            crawl_time = datetime.fromisoformat(crawl_time)

        # content ì„ë² ë”© ìƒì„±
        embedding_vector = embedding_model.embed_query(content)

        cur.execute(
            "INSERT INTO crawled_data (title, url, crawl_time, content, content_embedding) VALUES (%s, %s, %s, %s, %s::vector)",
            (title, url, crawl_time, content, embedding_vector)
        )

    conn.commit()
    cur.close()
    conn.close()

# =========================
# ì–¸ì–´ ê°ì§€
# =========================
def detect_language(text: str) -> str:
    if any('\uac00' <= ch <= '\ud7a3' for ch in text):
        return "ko"
    return "en"

# =========================
# PostgreSQL ê¸°ë°˜ RAG ë¦¬íŠ¸ë¦¬ë²„
# =========================
def retrieve_documents(question: str, db_name: str = "bbot_db", top_k: int = 5):
    q_embedding = embedding_model.embed_query(question)
    
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        dbname=db_name,
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD"),
        port=os.getenv("DB_PORT")
    )
    cur = conn.cursor()

    
    cur.execute("""
        SELECT title, url, content
        FROM crawled_data
        ORDER BY content_embedding <#> %s::vector
        LIMIT %s
    """, (q_embedding, top_k))

    results = cur.fetchall()
    cur.close()
    conn.close()
    
    return documents

# =========================
# RAG ë‹µë³€ ìƒì„±
# =========================
def generate(question: str, use_rag: bool = True) -> str:
    lang = detect_language(question)
    lang_instruction = "ì‚¬ìš©ì ì§ˆë¬¸ì´ í•œêµ­ì–´ì´ë¯€ë¡œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”." if lang=="ko" else "Answer naturally in English."

    context_text = ""
    if use_rag:
        docs = retrieve_documents(question)
        for d in docs:
            context_text += f"Title: {d['title']}\nContent: {d['content']}\nURL: {d['url']}\n\n"

    system_prompt = f"""
    ë‹¹ì‹ ì€ ê¸°ë…êµì  ê´€ì ì—ì„œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì§ˆë¬¸ì´ ì¼ë°˜ì ì´ê±°ë‚˜ ê³¼í•™ì ì´ì–´ë„, ë‹µë³€ì— ë°˜ë“œì‹œ ì„±ê²½ì  ê´€ì ì„ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
    ë§ˆì§€ë§‰ì— ì¶œì²˜ë„ ë°˜ì˜í•˜ëŠ”ë° ì°¸ì¡°í•œ ë°ì´í„°ì˜ urlì…ë‹ˆë‹¤.

    {lang_instruction}

    ì°¸ê³  ë¬¸ì„œ:
    {context_text}
    """

    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ],
        temperature=0
    )
    return response.choices[0].message.content

# =========================
# Question Rewriter
# =========================
system_rewriter = """ë‹¹ì‹ ì€ ì…ë ¥ëœ ì§ˆë¬¸ì„ ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ì— ìµœì í™”ëœ ë” ë‚˜ì€ ë²„ì „ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì§ˆë¬¸ ì¬ì‘ì„±ìì…ë‹ˆë‹¤."""

prompt_rewriter = ChatPromptTemplate.from_messages([
    ("system", system_rewriter),
    ("human", "Original question: {question}")
])

def upstage_rewriter(prompt_value):
    prompt_text = prompt_value.to_string()
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "system", "content": system_rewriter}, {"role": "user", "content": prompt_text}],
        temperature=0
    )
    return response.choices[0].message.content

chain_rewriter = prompt_rewriter | RunnableLambda(upstage_rewriter) | StrOutputParser()

# =========================
# Relevancy íŒë‹¨
# =========================
class Relevancy(BaseModel):
    judgement: str
    binary_score: str

def is_relevant(question: str, document: str) -> Relevancy:
    system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ ì‹¬ì‚¬ìœ„ì›ì…ë‹ˆë‹¤. ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤."""
    prompt = f"""{system_prompt}\n\nRetrieved document:\n{document}\n\nUser question:\n{question}\n\nRespond in JSON format: {{"judgement": "relevant"/"not_relevant","binary_score": "yes"/"no"}}"""
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return Relevancy(**json.loads(response.choices[0].message.content))

# =========================
# Factfulness íŒë‹¨
# =========================
class Factfulness(BaseModel):
    judgement: str
    binary_score: str

def check_factfulness(document: str, generation: str) -> Factfulness:
    system_prompt = """ë‹¹ì‹ ì€ LLMì´ ìƒì„±í•œ ë‚´ìš©ì´ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì— ê·¼ê±°í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì‹¬ì‚¬ìœ„ì›ì…ë‹ˆë‹¤. ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤."""
    prompt = f"{system_prompt}\n\nSet of facts:\n{document}\n\nLLM generation:\n{generation}\n\nRespond in JSON format: {{'judgement':'factual'/'hallucinated','binary_score':'yes'/'no'}}"
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return Factfulness(**json.loads(response.choices[0].message.content))

# =========================
# State ì •ì˜
# =========================
class State(TypedDict):
    question: str
    generation: str
    documents: List[str]
    source: str