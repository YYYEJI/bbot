import os
import json
from datetime import datetime
from typing import List
from pydantic import BaseModel
from typing_extensions import TypedDict
from dotenv import load_dotenv
from openai import OpenAI
import psycopg2
from langchain_upstage import UpstageEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# üîπ ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú
load_dotenv()
api_key = os.getenv("UPSTAGE_API_KEY")
base_url = os.getenv("UPSTAGE_BASE_URL")

# üîπ Upstage Î™®Îç∏
model = OpenAI(api_key=api_key, base_url=base_url)
embedding_model = UpstageEmbeddings(upstage_api_key=api_key, model="embedding-query")

# =========================
# DB ÏÉùÏÑ± Î∞è Î¨∏ÏÑú Ï≤òÎ¶¨
# =========================
def create_db(metas: List[dict], db_name: str = "bbot_db") -> None:
    """Î©îÌÉÄ Îç∞Ïù¥ÌÑ∞Î•º PostgreSQL DBÏóê Ï†ÄÏû•, content_embeddingÏùÄ vectorÎ°ú Ï†ÄÏû•"""
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        dbname=db_name,
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD"),
        port=os.getenv("DB_PORT")
    )
    cur = conn.cursor()
    print("DB Ïó∞Í≤∞ ÏÑ±Í≥µ")

    # pgvector ÌôïÏû• ÏÉùÏÑ±
    cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")

    # ÌÖåÏù¥Î∏î ÏÉùÏÑ±
    cur.execute("""
    CREATE TABLE IF NOT EXISTS crawled_data (
        id SERIAL PRIMARY KEY,
        title TEXT,
        url TEXT,
        crawl_time TIMESTAMP,
        content TEXT,
        content_embedding vector(4096)
    );
    """)

    # DBÏóê Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ
    for m in metas:
        title = m.get("Title", "")
        url = m.get("URL", "")
        crawl_time = m.get("CrawlTime", datetime.now())
        content = m.get("Content", "")

        if isinstance(crawl_time, str):
            crawl_time = datetime.fromisoformat(crawl_time)

        # content ÏûÑÎ≤†Îî© ÏÉùÏÑ±
        embedding_vector = embedding_model.embed_query(content)

        cur.execute(
            "INSERT INTO crawled_data (title, url, crawl_time, content, content_embedding) VALUES (%s, %s, %s, %s, %s::vector)",
            (title, url, crawl_time, content, embedding_vector)
        )

    conn.commit()
    cur.close()
    conn.close()

# =========================
# Ïñ∏Ïñ¥ Í∞êÏßÄ
# =========================
def detect_language(text: str) -> str:
    if any('\uac00' <= ch <= '\ud7a3' for ch in text):
        return "ko"
    return "en"

# =========================
# PostgreSQL Í∏∞Î∞ò RAG Î¶¨Ìä∏Î¶¨Î≤Ñ
# =========================
def retrieve_documents(question: str, db_name: str = "bbot_db", top_k: int = 5):
    q_embedding = embedding_model.embed_query(question)
    
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        dbname=db_name,
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD"),
        port=os.getenv("DB_PORT")
    )
    cur = conn.cursor()

    
    cur.execute("""
        SELECT title, url, content
        FROM crawled_data
        ORDER BY content_embedding <#> %s::vector
        LIMIT %s
    """, (q_embedding, top_k))

    results = cur.fetchall()
    cur.close()
    conn.close()
    
    documents = [{"title": r[0], "url": r[1], "content": r[2]} for r in results]
    return documents

# =========================
# RAG ÎãµÎ≥Ä ÏÉùÏÑ±
# =========================
def generate(question: str, use_rag: bool = True) -> str:
    lang = detect_language(question)
    lang_instruction = "ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏Ïù¥ ÌïúÍµ≠Ïñ¥Ïù¥ÎØÄÎ°ú ÌïúÍµ≠Ïñ¥Î°ú ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎãµÎ≥ÄÌïòÏÑ∏Ïöî." if lang=="ko" else "Answer naturally in English."

    context_text = ""
    if use_rag:
        docs = retrieve_documents(question)
        for d in docs:
            context_text += f"Title: {d['title']}\nContent: {d['content']}\nURL: {d['url']}\n\n"

    system_prompt = f"""
    ÎãπÏã†ÏùÄ Í∏∞ÎèÖÍµêÏ†Å Í¥ÄÏ†êÏóêÏÑú ÎãµÎ≥ÄÌïòÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.
    ÏßàÎ¨∏Ïù¥ ÏùºÎ∞òÏ†ÅÏù¥Í±∞ÎÇò Í≥ºÌïôÏ†ÅÏù¥Ïñ¥ÎèÑ, ÎãµÎ≥ÄÏóê Î∞òÎìúÏãú ÏÑ±Í≤ΩÏ†Å Í¥ÄÏ†êÏùÑ Î∞òÏòÅÌï¥Ïïº Ìï©ÎãàÎã§.
    ÎßàÏßÄÎßâÏóê Ï∂úÏ≤òÎèÑ Î∞òÏòÅÌïòÎäîÎç∞ Ï∞∏Ï°∞Ìïú Îç∞Ïù¥ÌÑ∞Ïùò urlÏûÖÎãàÎã§.

    {lang_instruction}

    Ï∞∏Í≥† Î¨∏ÏÑú:
    {context_text}
    """

    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ],
        temperature=0
    )
    return response.choices[0].message.content

# =========================
# Question Rewriter
# =========================
system_rewriter = """You are a question re-writer that converts an input question
to a better version optimized for vectorstore retrieval."""

prompt_rewriter = ChatPromptTemplate.from_messages([
    ("system", system_rewriter),
    ("human", "Original question: {question}")
])

def upstage_rewriter(prompt_value):
    prompt_text = prompt_value.to_string()
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "system", "content": system_rewriter}, {"role": "user", "content": prompt_text}],
        temperature=0
    )
    return response.choices[0].message.content

chain_rewriter = prompt_rewriter | RunnableLambda(upstage_rewriter) | StrOutputParser()

# =========================
# Relevancy ÌåêÎã®
# =========================
class Relevancy(BaseModel):
    judgement: str
    binary_score: str

def is_relevant(question: str, document: str) -> Relevancy:
    system_prompt = """You are an expert judge assessing the relevance of a document to a user question.
Respond strictly in valid JSON only."""
    prompt = f"""{system_prompt}\n\nRetrieved document:\n{document}\n\nUser question:\n{question}\n\nRespond in JSON format: {{"judgement": "relevant"/"not_relevant","binary_score": "yes"/"no"}}"""
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return Relevancy(**json.loads(response.choices[0].message.content))

# =========================
# Factfulness ÌåêÎã®
# =========================
class Factfulness(BaseModel):
    judgement: str
    binary_score: str

def check_factfulness(document: str, generation: str) -> Factfulness:
    system_prompt = """You are a judge assessing whether an LLM generation is grounded in a set of retrieved documents.
Respond strictly in valid JSON only."""
    prompt = f"{system_prompt}\n\nSet of facts:\n{document}\n\nLLM generation:\n{generation}\n\nRespond in JSON format: {{'judgement':'factual'/'hallucinated','binary_score':'yes'/'no'}}"
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return Factfulness(**json.loads(response.choices[0].message.content))

# =========================
# State Ï†ïÏùò
# =========================
class State(TypedDict):
    question: str
    generation: str
    documents: List[str]
    source: str