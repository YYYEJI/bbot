import os
import json
from datetime import datetime
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_upstage import UpstageEmbeddings
from typing import List
from pydantic import BaseModel, Field
from typing_extensions import TypedDict
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# üîπ ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú
api_key = os.environ["UPSTAGE_API_KEY"]
base_url = os.environ["UPSTAGE_BASE_URL"]

# üîπ Upstage Î™®Îç∏
from openai import OpenAI
model = OpenAI(api_key=api_key, base_url=base_url)

# =========================
# DB ÏÉùÏÑ± Î∞è Î¨∏ÏÑú Ï≤òÎ¶¨
# =========================
def create_db(metas: List[dict], persist_dir: str = "./chroma_db") -> Chroma:
    """Î©îÌÉÄ Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Chroma VectorStore ÏÉùÏÑ±"""
    docs = [
        Document(
            page_content=m["Content"],
            metadata={"URL": m["URL"], "Title": m["Title"], "CrawlTime": m["CrawlTime"]}
        )
        for m in metas
    ]
    # ÌÖçÏä§Ìä∏ Î∂ÑÌï†
    splitter = RecursiveCharacterTextSplitter(chunk_size=3500, chunk_overlap=100)
    split_docs = splitter.split_documents(docs)
    
    # ÏûÑÎ≤†Îî© Î™®Îç∏
    embedding_model = UpstageEmbeddings(upstage_api_key=api_key, model="embedding-query")
    
    # Chroma DB
    db = Chroma.from_documents(documents=split_docs, embedding=embedding_model, persist_directory=persist_dir)
    return db

# =========================
# RAG Í¥ÄÎ†® Ìï®Ïàò
# =========================
def web_search(question: str) -> str:
    # Ïõπ Í≤ÄÏÉâ placeholder
    return "Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º ÌÖçÏä§Ìä∏"

def generate(question: str) -> str:
    """
    - ÏùºÎ∞ò ÏßàÎ¨∏ÏùÄ LLMÏóêÍ≤å ÎãµÎ≥Ä
    - Î™®Îì† ÎãµÎ≥ÄÏóê Í∏∞ÎèÖÍµêÏ†Å Í¥ÄÏ†êÏùÑ Ìè¨Ìï®
    """
    system_prompt = """
    ÎãπÏã†ÏùÄ Í∏∞ÎèÖÍµêÏ†Å Í¥ÄÏ†êÏóêÏÑú ÎãµÎ≥ÄÌïòÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.
    ÏßàÎ¨∏Ïù¥ ÏùºÎ∞òÏ†ÅÏù¥Í±∞ÎÇò Í≥ºÌïôÏ†ÅÏù¥Ïñ¥ÎèÑ, ÎãµÎ≥ÄÏóê Î∞òÎìúÏãú ÏÑ±Í≤ΩÏ†Å ÎòêÎäî Í∏∞ÎèÖÍµêÏ†Å Í¥ÄÏ†êÏùÑ Î∞òÏòÅÌï¥Ïïº Ìï©ÎãàÎã§.
    """
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ],
        temperature=0
    )
    return response.choices[0].message.content

# =========================
# Question Rewriting
# =========================
system = """You are a question re-writer that converts an input question
to a better version optimized for vectorstore retrieval.
Analyze the input and reason about the underlying semantic intent/need of the question, then rewrite it accordingly."""

prompt_rewriter = ChatPromptTemplate.from_messages([
    ("system", system),
    ("human", "Original question: {question}")
])

def upstage_rewriter(prompt_value):
    prompt_text = prompt_value.to_string()
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "system", "content": system}, {"role": "user", "content": prompt_text}],
        temperature=0
    )
    return response.choices[0].message.content

chain_rewriter = prompt_rewriter | RunnableLambda(upstage_rewriter) | StrOutputParser()

# =========================
# Relevancy ÌåêÎã®
# =========================
class Relevancy(BaseModel):
    judgement: str
    binary_score: str

def is_relevant(question: str, document: str) -> Relevancy:
    system_prompt = """You are an expert judge assessing the relevance of a document to a user question.
Respond strictly in valid JSON only."""
    prompt = f"""{system_prompt}\n\nRetrieved document:\n{document}\n\nUser question:\n{question}\n\nRespond in JSON format: {{"judgement": "relevant"/"not_relevant","binary_score": "yes"/"no"}}"""
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    data = json.loads(response.choices[0].message.content)
    return Relevancy(**data)

# =========================
# Factfulness ÌåêÎã®
# =========================
class Factfulness(BaseModel):
    judgement: str
    binary_score: str

def check_factfulness(document: str, generation: str) -> Factfulness:
    system_prompt = """You are a judge assessing whether an LLM generation is grounded in a set of retrieved documents.
Respond strictly in valid JSON only."""
    prompt = f"{system_prompt}\n\nSet of facts:\n{document}\n\nLLM generation:\n{generation}\n\nRespond in JSON format: {{'judgement':'factual'/'hallucinated','binary_score':'yes'/'no'}}"
    response = model.chat.completions.create(
        model="solar-pro2",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    data = json.loads(response.choices[0].message.content)
    return Factfulness(**data)

# =========================
# State Ï†ïÏùò
# =========================
class State(TypedDict):
    question: str
    generation: str
    documents: List[str]
    source: str